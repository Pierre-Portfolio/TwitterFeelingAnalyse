{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5dc2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e200e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\owen9\\OneDrive\\Documents\\GitHub\\TwitterFeelingAnalyse\\Dataset\\twitter_data_161122.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77498774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-16 23:59:43+00:00</td>\n",
       "      <td>1593031089961869315</td>\n",
       "      <td>@Guy_Smiley_73 @houndfucker @Indiekindoflife @...</td>\n",
       "      <td>julinnette1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-16 23:59:33+00:00</td>\n",
       "      <td>1593031045934301185</td>\n",
       "      <td>GL:Well, don’t commit suicide.「自殺しないことです」(S1E1)</td>\n",
       "      <td>Sherlock_meigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-16 23:59:20+00:00</td>\n",
       "      <td>1593030994600538112</td>\n",
       "      <td>@RealGomezMedia @iblockliberals @WickedR3coil ...</td>\n",
       "      <td>SIRLIGHTANGEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-16 23:59:06+00:00</td>\n",
       "      <td>1593030935087575041</td>\n",
       "      <td>@Brjl_ おはよ！</td>\n",
       "      <td>Suicide429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-16 23:59:04+00:00</td>\n",
       "      <td>1593030924585021440</td>\n",
       "      <td>whilst i do not encourage misogyny: misandry k...</td>\n",
       "      <td>darthpilled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime             Tweet Id  \\\n",
       "0  2022-11-16 23:59:43+00:00  1593031089961869315   \n",
       "1  2022-11-16 23:59:33+00:00  1593031045934301185   \n",
       "2  2022-11-16 23:59:20+00:00  1593030994600538112   \n",
       "3  2022-11-16 23:59:06+00:00  1593030935087575041   \n",
       "4  2022-11-16 23:59:04+00:00  1593030924585021440   \n",
       "\n",
       "                                                Text         Username  \n",
       "0  @Guy_Smiley_73 @houndfucker @Indiekindoflife @...      julinnette1  \n",
       "1    GL:Well, don’t commit suicide.「自殺しないことです」(S1E1)  Sherlock_meigen  \n",
       "2  @RealGomezMedia @iblockliberals @WickedR3coil ...    SIRLIGHTANGEL  \n",
       "3                                        @Brjl_ おはよ！       Suicide429  \n",
       "4  whilst i do not encourage misogyny: misandry k...      darthpilled  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_init = pd.read_csv(path,sep=',',encoding='utf-8')\n",
    "df_init.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e5884a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_init.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a02c1",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fefd82c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa43a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predire(text):\n",
    "    '''\n",
    "    if type=='adverbe':\n",
    "        Ltag=['RB','RBR','RBS','RBW']\n",
    "        senti='r'\n",
    "    elif type=='verbe':\n",
    "        Ltag=['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "        senti='v'\n",
    "    '''\n",
    "\n",
    "    train_sents = treebank.tagged_sents()[:3000]\n",
    "    tagger = UnigramTagger(train_sents)\n",
    "\n",
    "    texte=word_tokenize(text)\n",
    "    L=set()\n",
    "    tag=tagger.tag(texte)\n",
    "\n",
    "    for t in tag:\n",
    "        L.add(t[0])\n",
    "        \n",
    "    pos_score=0\n",
    "    neg_score=0\n",
    "\n",
    "    print(L)\n",
    "    for adv in L:\n",
    "        adv_senti = list(swn.senti_synsets(adv))\n",
    "        if len(adv_senti)!=0:\n",
    "            pos_score+=adv_senti[0].pos_score()\n",
    "            neg_score+=adv_senti[0].neg_score()\n",
    "\n",
    "\n",
    "    res=''\n",
    "    if pos_score > neg_score:\n",
    "        res='pos'\n",
    "    elif pos_score < neg_score:\n",
    "        res='neg'\n",
    "\n",
    "    return res,pos_score,neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a791f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Guy_Smiley_73 @houndfucker @Indiekindoflife @AlessandraVaron But they do! They’re the reason that the suicide rate in the lgbt community is higher and that’s just a fact\n",
      "{'they', 'AlessandraVaron', 'a', 'community', 'Indiekindoflife', 's', 'just', '’', '@', 'higher', 're', 'They', 'that', 'is', 'But', 'suicide', '!', 'the', 'do', 'rate', 'Guy_Smiley_73', 'in', 'and', 'reason', 'lgbt', 'houndfucker', 'fact'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('pos', 1.375, 0.25)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=df.loc[0]['Text']\n",
    "print(test)\n",
    "Predire(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('--------------')\n",
    "    print(df.loc[i]['Text'])\n",
    "    print(Predire(df.loc[i]['Text']))\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3494522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=  18992\n",
      "7117 6991 4884\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "neutre=0\n",
    "for index,row in df.iterrows():\n",
    "    if Predire(row['Text'])[0] == 'pos':\n",
    "        pos+=1\n",
    "    elif Predire(row['Text'])[0] == 'neg':\n",
    "        neg+=1\n",
    "    else:\n",
    "        neutre+=1\n",
    "\n",
    "print('total= ', len(df))\n",
    "print(pos,neg,neutre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98565d43",
   "metadata": {},
   "source": [
    "# Valder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ea972",
   "metadata": {},
   "source": [
    "Lien pour les 3 modèles: \n",
    "\n",
    "https://towardsdatascience.com/the-most-favorable-pre-trained-sentiment-classifiers-in-python-9107c06442c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82829804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install vaderSentimentlibrary\n",
    "#!pip install vaderSentiment\n",
    "\n",
    "#import the library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a53dd222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the negative, positive, neutral and compound scores, plus verbal evaluation\n",
    "def sentiment_vader(sentence):\n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    negative = sentiment_dict['neg']\n",
    "    neutral = sentiment_dict['neu']\n",
    "    positive = sentiment_dict['pos']\n",
    "    compound = sentiment_dict['compound']\n",
    "\n",
    "    if sentiment_dict['compound'] <= - 0.05 :\n",
    "        overall_sentiment = \"Negative\"\n",
    "        \n",
    "    elif sentiment_dict['compound'] >= 0.05 :\n",
    "        overall_sentiment = \"Positive\"   \n",
    "\n",
    "    else :\n",
    "        overall_sentiment = \"Neutral\"\n",
    "  \n",
    "    return negative, neutral, positive, compound, overall_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ada3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Guy_Smiley_73 @houndfucker @Indiekindoflife @AlessandraVaron But they do! They’re the reason that the suicide rate in the lgbt community is higher and that’s just a fact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.214, 0.786, 0.0, -0.8197, 'Negative')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.loc[0,'Text'])\n",
    "sentiment_vader(df.loc[0,'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1deec116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=  18992\n",
      "2470 12563 3959\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "neutre=0\n",
    "for index,row in df.iterrows():\n",
    "    if sentiment_vader(row['Text'])[4] == 'Positive':\n",
    "        pos+=1\n",
    "    elif sentiment_vader(row['Text'])[4] == 'Negative':\n",
    "        neg+=1\n",
    "    else:\n",
    "        neutre+=1\n",
    "\n",
    "print('total= ', len(df))\n",
    "print(pos,neg,neutre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dfac92",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "050d0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install textblob\n",
    "#pip install textblob\n",
    "\n",
    "# import it\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df99e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the classifier\n",
    "def sentiment_texblob(row):\n",
    "  \n",
    "    classifier = TextBlob(row)\n",
    "    polarity = classifier.sentiment.polarity #[-1,1] emotions\n",
    "    subjectivity = classifier.sentiment.subjectivity # objectif [0,1] subjectif\n",
    "    \n",
    "    return polarity,subjectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d889fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Guy_Smiley_73 @houndfucker @Indiekindoflife @AlessandraVaron But they do! They’re the reason that the suicide rate in the lgbt community is higher and that’s just a fact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 0.5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.loc[0,'Text'])\n",
    "sentiment_texblob(df.loc[0,'Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdde3ed",
   "metadata": {},
   "source": [
    "# Happy Transormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43e229b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.core.framework.types_pb2' has no attribute 'SerializedDType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2404\\3199434852.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#!pip install happytransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhappytransformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHappyTextClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\happytransformer\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhappytransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhappy_question_answering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHappyQuestionAnswering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhappytransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhappy_word_prediction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHappyWordPrediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhappytransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhappy_text_classification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHappyTextClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhappytransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhappy_next_sentence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHappyNextSentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhappytransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhappy_token_classification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHappyTokenClassification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\happytransformer\\happy_question_answering.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQuestionAnsweringPipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModelForQuestionAnswering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhappytransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhappy_transformer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHappyTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m from .utils import (\n\u001b[0;32m     32\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\transformers\\dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpkg\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tokenizers\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# must be loaded here, or else tqdm check may fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\transformers\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[1;33m from .generic import (\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# from tensorflow.python import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    417\u001b[0m \"\"\"\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_service_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# ==============================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;34m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrapt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \"\"\"\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sparse_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dtypes.DType\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DType\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m class DType(\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0m_dtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTraceType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Logiciels\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36mDType\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m   \u001b[1;32mdef\u001b[0m \u001b[0mexperimental_type_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtypes_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializedDType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[1;34m\"\"\"Returns the type of proto associated with DType serialization.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializedDType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.core.framework.types_pb2' has no attribute 'SerializedDType'"
     ]
    }
   ],
   "source": [
    "#!pip install happytransformer\n",
    "\n",
    "from happytransformer import HappyTextClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4474e9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6991ab373c3a4acf90a1a9dc55215b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af27e5cb6a745aa978f0a2f422a6710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb41ba6eea584f9180ae5fa81454c5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a520ff82ff4b412ab49c735edc4878f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/08/2022 18:28:27 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    }
   ],
   "source": [
    "happy_tc = HappyTextClassification(model_type=\"DISTILBERT\", model_name=\"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82500786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Guy_Smiley_73 @houndfucker @Indiekindoflife @AlessandraVaron But they do! They’re the reason that the suicide rate in the lgbt community is higher and that’s just a fact\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextClassificationResult(label='NEGATIVE', score=0.9565466642379761)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.loc[0,'Text'])\n",
    "happy_tc.classify_text(df.loc[0,'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c482f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=  18992\n",
      "2208 16784 0\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "neutre=0\n",
    "for index,row in df.iterrows():   \n",
    "    if happy_tc.classify_text(row['Text']).label == 'NEGATIVE':\n",
    "        neg+=1\n",
    "    elif happy_tc.classify_text(row['Text']).label == 'POSITIVE':\n",
    "        pos+=1\n",
    "    else:\n",
    "        neutre+=1\n",
    "\n",
    "print('total= ', len(df))\n",
    "print(pos,neg,neutre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f4f80",
   "metadata": {},
   "source": [
    "# Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96620770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "833930e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df2=df_init.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1bd7e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1220.3325100000002\n"
     ]
    }
   ],
   "source": [
    "t1=time.perf_counter()\n",
    "\n",
    "df2['Label_NLP']=df2.apply(lambda row: Predire(row['Text'])[0], axis=1)\n",
    "df2['Label_Valder']=df2.apply(lambda row: sentiment_vader(row['Text'])[4], axis=1)\n",
    "df2['Polarity']=df2.apply(lambda row: sentiment_texblob(row['Text'])[0], axis=1)\n",
    "df2['Subjectivity']=df2.apply(lambda row: sentiment_texblob(row['Text'])[1], axis=1)\n",
    "df2['Label_happy_transformer']=df2.apply(lambda row: happy_tc.classify_text(row['Text']).label, axis=1)\n",
    "\n",
    "t2=time.perf_counter()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92642d91",
   "metadata": {},
   "source": [
    "20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed2b6dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Label_NLP</th>\n",
       "      <th>Label_Valder</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Label_happy_transformer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-16 23:59:43+00:00</td>\n",
       "      <td>1593031089961869315</td>\n",
       "      <td>@Guy_Smiley_73 @houndfucker @Indiekindoflife @...</td>\n",
       "      <td>julinnette1</td>\n",
       "      <td>pos</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.375</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-16 23:59:33+00:00</td>\n",
       "      <td>1593031045934301185</td>\n",
       "      <td>GL:Well, don’t commit suicide.「自殺しないことです」(S1E1)</td>\n",
       "      <td>Sherlock_meigen</td>\n",
       "      <td>pos</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-16 23:59:20+00:00</td>\n",
       "      <td>1593030994600538112</td>\n",
       "      <td>@RealGomezMedia @iblockliberals @WickedR3coil ...</td>\n",
       "      <td>SIRLIGHTANGEL</td>\n",
       "      <td>neg</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-16 23:59:06+00:00</td>\n",
       "      <td>1593030935087575041</td>\n",
       "      <td>@Brjl_ おはよ！</td>\n",
       "      <td>Suicide429</td>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-16 23:59:04+00:00</td>\n",
       "      <td>1593030924585021440</td>\n",
       "      <td>whilst i do not encourage misogyny: misandry k...</td>\n",
       "      <td>darthpilled</td>\n",
       "      <td>pos</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.875</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime             Tweet Id  \\\n",
       "0  2022-11-16 23:59:43+00:00  1593031089961869315   \n",
       "1  2022-11-16 23:59:33+00:00  1593031045934301185   \n",
       "2  2022-11-16 23:59:20+00:00  1593030994600538112   \n",
       "3  2022-11-16 23:59:06+00:00  1593030935087575041   \n",
       "4  2022-11-16 23:59:04+00:00  1593030924585021440   \n",
       "\n",
       "                                                Text         Username  \\\n",
       "0  @Guy_Smiley_73 @houndfucker @Indiekindoflife @...      julinnette1   \n",
       "1    GL:Well, don’t commit suicide.「自殺しないことです」(S1E1)  Sherlock_meigen   \n",
       "2  @RealGomezMedia @iblockliberals @WickedR3coil ...    SIRLIGHTANGEL   \n",
       "3                                        @Brjl_ おはよ！       Suicide429   \n",
       "4  whilst i do not encourage misogyny: misandry k...      darthpilled   \n",
       "\n",
       "  Label_NLP Label_Valder  Polarity  Subjectivity Label_happy_transformer  \n",
       "0       pos     Negative     0.250         1.375                NEGATIVE  \n",
       "1       pos     Positive     0.000         0.125                NEGATIVE  \n",
       "2       neg     Positive     0.600         0.000                NEGATIVE  \n",
       "3                Neutral     0.000         0.000                NEGATIVE  \n",
       "4       pos     Negative     0.125         2.875                POSITIVE  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
