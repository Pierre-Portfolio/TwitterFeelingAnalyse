{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b651ce",
   "metadata": {},
   "source": [
    "# Twitter Detection Suicide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a87745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies to install\n",
    "!pip install vaderSentiment\n",
    "!pip install textblob\n",
    "!pip install happytransformer\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f87b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important librairy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Modele\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from happytransformer import HappyTextClassification\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Analyse des données\n",
    "import seaborn as sn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ed014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les deux dataset de test et de train\n",
    "df = pd.read_csv('./Suicide_Detection_Reddit2.csv')\n",
    "#df = pd.read_csv('./Suicide_Detection_Tweeter.csv')\n",
    "#df = pd.read_csv(r'C:\\Users\\owen9\\OneDrive\\Documents\\GitHub\\TwitterFeelingAnalyse\\Dataset\\Suicide_Detection_Tweeter.csv')\n",
    "df = df.rename(columns={\n",
    "    \"Tweet\": \"text\",\n",
    "    \"Suicide\": \"class\"\n",
    "})\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "df[\"class\"] = df[\"class\"].astype(str)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad74af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche la dimension du dataset\n",
    "print('Nombre de ligne : ' +  str(df.shape[0]))\n",
    "print('Nombre de colonne : ' + str(df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d3b10",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cleaning(Text):\n",
    "    #supprimer les identifiactions avec les @\n",
    "    L=[]#liste des mots à enlever\n",
    "    for i in range(len(Text)):\n",
    "        if Text[i]=='@':            \n",
    "            s=''\n",
    "            for j in Text[i:]:\n",
    "                if j != ' ':\n",
    "                    s+=j\n",
    "                else:\n",
    "                    break            \n",
    "            L.append(s)\n",
    "    #suppression de tous les mots\n",
    "    for word in L:\n",
    "        Text=Text.replace(word,'')\n",
    "    \n",
    "    #On garde que les caractères alphanumériques\n",
    "    Text=re.sub(r'((?![a-z]|[A-Z]|\\d).)',' ',Text)#((?!...).) => opposé\n",
    "    \n",
    "    return Text.strip() #suppression des espaces au début et à la fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe94567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: Cleaning(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6da791",
   "metadata": {},
   "source": [
    "# Modeles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d5a1a",
   "metadata": {},
   "source": [
    "## Fonction Globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ddb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciez le modèle et le tokeniseur\n",
    "def Classificator(name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    model = AutoModel.from_pretrained(name)\n",
    "    return pipeline(model=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword Analyse\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)\n",
    "\n",
    "# Classificateur\n",
    "happy_tc = HappyTextClassification(model_type=\"DISTILBERT\", model_name=\"distilbert-base-uncased-finetuned-sst-2-english\", num_labels=2)\n",
    "classifier_bert = Classificator(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "classifier_suicidal_bert = Classificator(\"gooohjy/suicidal-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddModeleIntoDataframe(df, function_modele, list_column_name):\n",
    "    df[list_column_name] = df['text'].apply(lambda x: function_modele(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddAllModelToDF(df):\n",
    "    #cleaning du dataframe\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: x[:500])\n",
    "    \n",
    "    print(\"Réalisation de la fonction keyword_analyse en cours ....\")\n",
    "    #df = AddModeleIntoDataframe(df, sentiment_keyword, \"keyword_label\")\n",
    "    \n",
    "    print(\"Réalisation de la fonction sentiment_vader en cours ....\")\n",
    "    df = AddModeleIntoDataframe(df, sentiment_vader, \"valder_label\")\n",
    "    \n",
    "    print(\"Réalisation de la fonction sentiment_texblob en cours ....\")\n",
    "    df = AddModeleIntoDataframe(df, sentiment_texblob_polarity, \"textbloc_polarity_label\")\n",
    "    df = AddModeleIntoDataframe(df, sentiment_texblob_subjectivity, \"textbloc_subjectivity_label\")\n",
    "    \n",
    "    print(\"Réalisation de la fonction sentiment_happy_transformer en cours ....\")\n",
    "    df = AddModeleIntoDataframe(df, sentiment_happy_transformer, \"happy_transformer_label\")\n",
    "    \n",
    "    print(\"Réalisation de la fonction sentiment_bert en cours ....\")\n",
    "    df = AddModeleIntoDataframe(df, sentiment_bert, \"bert_label\")\n",
    "    \n",
    "    print(\"Réalisation de la fonction sentiment_suicidal_bert en cours ....\")\n",
    "    df = AddModeleIntoDataframe(df, sentiment_suicidal_bert, \"suicidal_bert_label\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a36d1",
   "metadata": {},
   "source": [
    "## KeyWord Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65400305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_keyword(text):\n",
    "    texte=word_tokenize(text)\n",
    "    L=set()\n",
    "    tag=tagger.tag(texte)\n",
    "\n",
    "    for t in tag:\n",
    "        L.add(t[0])\n",
    "\n",
    "    pos_score=0\n",
    "    neg_score=0\n",
    "\n",
    "    for adv in L:\n",
    "        adv_senti = list(swn.senti_synsets(adv))\n",
    "        if len(adv_senti)!=0:\n",
    "            pos_score+=adv_senti[0].pos_score()\n",
    "            neg_score+=adv_senti[0].neg_score()\n",
    "    res=''\n",
    "    \n",
    "    if pos_score > neg_score:\n",
    "        res='pos'\n",
    "    elif pos_score < neg_score:\n",
    "        res='neg'\n",
    "\n",
    "    return res,pos_score,neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cae359",
   "metadata": {},
   "source": [
    "## Valder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_vader(sentence):\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    negative = sentiment_dict['neg']\n",
    "    neutral = sentiment_dict['neu']\n",
    "    positive = sentiment_dict['pos']\n",
    "    compound = sentiment_dict['compound']\n",
    "\n",
    "    if sentiment_dict['compound'] <= - 0.05 :\n",
    "        overall_sentiment = \"Negative\"\n",
    "        \n",
    "    elif sentiment_dict['compound'] >= 0.05 :\n",
    "        overall_sentiment = \"Positive\"   \n",
    "\n",
    "    else :\n",
    "        overall_sentiment = \"Neutral\"\n",
    "  \n",
    "    return overall_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356f1fa",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64245b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_texblob_polarity(row):\n",
    "    classifier = TextBlob(row)\n",
    "    polarity = classifier.sentiment.polarity\n",
    "    return polarity\n",
    "\n",
    "def sentiment_texblob_subjectivity(row):\n",
    "    classifier = TextBlob(row)\n",
    "    subjectivity = classifier.sentiment.subjectivity\n",
    "    return subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1509569",
   "metadata": {},
   "source": [
    "## Happy Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ddeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_happy_transformer(row):\n",
    "    result = happy_tc.classify_text(row)\n",
    "    return str(result.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811695d",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f030b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_bert(row):\n",
    "    # On recupere le sentiment\n",
    "    res = classifier_bert(row)\n",
    "\n",
    "    # Ajoutez les résultats aux tableaux\n",
    "    return res[0]['label'][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8d981",
   "metadata": {},
   "source": [
    "## Suicidal BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_suicidal_bert(row):\n",
    "    # On recupere le sentiment\n",
    "    res = classifier_suicidal_bert(row)\n",
    "\n",
    "    # Ajoutez les résultats aux tableaux\n",
    "    return res[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a4f2b3",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreparationModel(df):\n",
    "    df[\"class\"] = df[\"class\"].replace({\"Potential Suicide post \": 1, \"Not Suicide post\": 0})\n",
    "    df[\"valder_label\"] = df[\"valder_label\"].replace({\"Negative\": 1, \"Positive\": 0, \"Neutral\": 0})\n",
    "    df[\"happy_transformer_label\"] = df[\"happy_transformer_label\"].replace({\"NEGATIVE\": 1, \"POSITIVE\": 0})\n",
    "    df[\"bert_label\"] = df[\"bert_label\"].replace({\"1\": 1, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0})\n",
    "    df[\"suicidal_bert_label\"] = df[\"suicidal_bert_label\"].replace({\"LABEL_1\": 1, \"LABEL_0\": 0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588375b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = AddAllModelToDF(df)\n",
    "df = PreparationModel(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous allons nous concentrez uniquement sur la caractéristique cible\n",
    "plt.figure(figsize=(20,20))\n",
    "heatmap = sn.heatmap(df.corr()[['class']].sort_values(by='class', ascending=False), vmin=-0.5, vmax=1, annot=True, cmap='Blues')\n",
    "heatmap.set_title('Correlation du résultats avec les autres features', pad=16, fontdict={'family': 'serif','size': 20});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(r'C:\\Users\\owen9\\OneDrive\\Documents\\GitHub\\TwitterFeelingAnalyse\\Dataset\\Resultats_Owen.csv', sep=';', index=False)\n",
    "#df.to_csv('./Suicide_Detection_Tweeter_Final.csv', index=False)\n",
    "df.to_csv('./Suicide_Detection_Reddit_Final.csv', index=False, ,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6f374",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Suicide_Detection_Tweeter_Final.csv',sep=';')\n",
    "# df = pd.read_csv('./Suicide_Detection_Reddit_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TauxBonneReponse(df):\n",
    "    # Sélection des lignes où la colonne \"class\" contient la valeur 1\n",
    "    selected_true_positive = df.loc[df['class'] == 1]\n",
    "    return [selected_true_positive.shape[0], df.shape[0], round(selected_true_positive.shape[0] / df.shape[0], 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3bef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genere un dataframe ou stock le meilleur modele pour chaque méthode\n",
    "ResultDf = pd.DataFrame()\n",
    "def StockResultDf(name, df):\n",
    "    global ResultDf\n",
    "    tab = TauxBonneReponse(df)\n",
    "    listValue = {'Methode Name': name, 'Nombre de bonne reponse': tab[0], 'Nombre de réponse totale': tab[1] , 'Taux de bonne réponse (%)': tab[2]}\n",
    "    ResultDf = ResultDf.append(listValue,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectionTweetSuicidaire(df):\n",
    "    #Methode Naif\n",
    "    StockResultDf(\"Naif\", df)\n",
    "    \n",
    "    #Methode 1\n",
    "    # On ne garde que suicidal bert\n",
    "    df1 = df[df['suicidal_bert_label'] == 1]\n",
    "    StockResultDf(\"Model 1\", df1)\n",
    "    \n",
    "    #Methode 2\n",
    "    # Sélection des lignes où au moins 3 des colonnes ont la valeur 1\n",
    "    df2 = df[(df[['valder_label', 'happy_transformer_label', 'bert_label', 'suicidal_bert_label']].sum(axis=1) >= 3)]\n",
    "    StockResultDf(\"Model 2\", df2)\n",
    "    \n",
    "#On execute puis on affiche le resultat\n",
    "SelectionTweetSuicidaire(df.copy())\n",
    "ResultDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d83429",
   "metadata": {},
   "source": [
    "# Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927dcd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init=pd.read_csv(r'C:\\Users\\owen9\\OneDrive\\Documents\\GitHub\\TwitterFeelingAnalyse\\Dataset\\Resultats_Owen.csv',sep=';')\n",
    "df=df_init.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init=pd.read_csv(r'C:\\Users\\owen9\\OneDrive\\Documents\\GitHub\\TwitterFeelingAnalyse\\Dataset\\Suicide_Detection_Reddit_Final.csv')\n",
    "df=df_init.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5584ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valder\n",
    "matrix_valder = confusion_matrix(y_true=df['class'], y_pred=df['valder_label'])\n",
    "matrix_valder = matrix_valder/(np.sum(matrix_valder))\n",
    "df_matrix_valder = pd.DataFrame(matrix_valder, ['negatif','positif'], ['négatif','positif'])\n",
    "\n",
    "#Happy transformer\n",
    "matrix_hf = confusion_matrix(y_true=df['class'], y_pred=df['happy_transformer_label'])\n",
    "matrix_hf = matrix_hf/(np.sum(matrix_hf))\n",
    "df_matrix_hf = pd.DataFrame(matrix_hf, ['negatif','positif'], ['négatif','positif'])\n",
    "\n",
    "#Bert\n",
    "matrix_bert = confusion_matrix(y_true=df['class'], y_pred=df['bert_label'])\n",
    "matrix_bert = matrix_bert/(np.sum(matrix_bert))\n",
    "df_matrix_bert = pd.DataFrame(matrix_bert, ['negatif','positif'], ['négatif','positif'])\n",
    "\n",
    "#Suicidal Bert\n",
    "matrix_sbert = confusion_matrix(y_true=df['class'], y_pred=df['suicidal_bert_label'])\n",
    "matrix_sbert = matrix_sbert/(np.sum(matrix_sbert))\n",
    "df_matrix_sbert = pd.DataFrame(matrix_sbert, ['negatif','positif'], ['négatif','positif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap=plt.cm.Blues\n",
    "\n",
    "fig, axs = plt.subplots(2, 2 ,figsize=(8,8))\n",
    "fig.suptitle('Matrices de confusion des modèles', fontsize=16)\n",
    "\n",
    "#Valder\n",
    "g1=sn.heatmap(df_matrix_valder, cmap=cmap, annot=True, fmt='.2%', cbar=False, vmin=0.2, vmax=0.6, ax=axs[0,0])\n",
    "axs[0,0].set_title('Valder')\n",
    "g1.set_ylabel('Labels')\n",
    "\n",
    "#Happy transormer\n",
    "g2=sn.heatmap(df_matrix_hf, cmap=cmap, annot=True, fmt='.2%',cbar=False, vmin=0.2, vmax=0.6, ax=axs[1,0])\n",
    "axs[1,0].set_title('Happy transformer')\n",
    "g2.set_xlabel('Prédictions')\n",
    "g2.set_ylabel('Labels')\n",
    "\n",
    "#Bert\n",
    "g3=sn.heatmap(df_matrix_bert, cmap=cmap, annot=True, fmt='.2%', vmin=0.2, vmax=0.6, ax=axs[0,1])\n",
    "axs[0,1].set_title('Bert')\n",
    "#g2.set_xlabel('Prédictions')\n",
    "#g2.set_ylabel('Labels')\n",
    "\n",
    "#Suicidal Bert\n",
    "g4=sn.heatmap(df_matrix_sbert, cmap=cmap, annot=True, fmt='.2%', vmin=0.2, vmax=0.6, ax=axs[1,1])\n",
    "axs[1,1].set_title('Suicidal Bert')\n",
    "g4.set_xlabel('Prédictions')\n",
    "#g2.set_ylabel('Labels')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
